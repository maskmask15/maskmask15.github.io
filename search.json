[{"title":"AI编译器学习记录 01","path":"/2025/09/25/AI编译器学习记录01/","content":"AI编译器学习记录 01, 跟随大佬的脚步 AI编译器发展过程 第一阶段朴素的AI编译器，tensorflow的早期模式。 第二阶段专用的AI编译器，pytorch以及类pytorch。将类pytorch的表达转换成图层的IR进行优化。希望能够打开计算图图和算子的边界进行重新组合优化。关键是大算子怎么打开，小算子怎么融合。 ZOMI大佬提出的一个问题很好，AI框架和AI编译器的界限在哪里？一个AI框架就是一个AI编译器么？还是AI编译器只是AI框架的一部分呢？ 在我的理解上，因为要提升所谓的推理性能，AI框架要内置AI编译器。但是AI框架不等于AI编译器，框架可调用编译器，编译器属于框架的后端。编译器可以独立与框架运行，比如TVM，TensorRT等。AI框架的硬件感知力较弱，需要编译器来针对特定目标平台做部署。 第三阶段通用的AI编译器图片取自ZOMI大佬 AI编译器的通用架构图片取自ZOMI大佬，AI编译器分为前端和后端，分别做硬件无关和硬件相关的处理。前端包括计算图的编译器以及算子的编译器。有不同的IR和不同的pass，和LLVM挺像的。 图 1：AI 编译器的整体架构示意图 IR High-level IR：用于表示计算图，表示神经网络。主要是为了解决传统编译器中难以表达深度学习模型中复杂运算这一问题，为了实现更高效的优化所以新设计了一套IR。比如TVM中的Relay IR。 Low-level IR：类似于传统编译器，能够在更细粒度的层面（比如代码、指令集层面）上表示模型，从而能够针对与硬件进行优化。比如TVM中的TIR。 前端优化构造计算图，前端进行图级的优化。图提供了计算全局概述，所以容易在图级发现和执行许多优化，且与硬件无关，这意味着任何后端目标都可应用计算图优化。 节点级优化，比如 Zero-dim-tensor dlimination（零维度张量消除）、 Nop Elimination。 块级优化，比如代数简化、常量折叠、算子融合。将多个算子变成一个块，变成一个子图的优化。 数据流级优化，比如Common sub-exoression elimination、DEC。更像是 Low-level IR上的优化。图片取自ZOMI大佬 面向图层的优化算子融合 OPFusion算子融合的本质就是要”减少访存 + 减少调度 + 提高并行”。融合的深度和粒度往往直接决定了最终的推理性能。 算子融合方式 水平融合，将输入相同，计算类型相同的算子融合为一个算子，在同一个kernel中执行。可减少访问输入数据的次数，提高并行度。 垂直融合，将前后相邻的算子链融合成一个算子，减少中间张量的存取。 算子内融合，将一系列小的逐元素的操作融合成一个大的复合算子。减少kernel的启动次数，存取次数。 图级融合，识别特定的计算子图模式，将其视为单一算子。可大幅度减少调度和存取， 算子融合示例，以经典的Conv+BN+Relu融合为例 第一种方式是我之前学算子融合时了解到的方式。就是将Conv+BN合成一个等价的带bias的卷积，再将ReLU合并进去。 卷积层输出为：$$y W * x + b$$ 融合的规则和算法 面向DAG图的优化后端优化 特定硬件的优化，获取高性能的代码 将 Low-level IR 转换为 LLVM IR ，利用LLVM基础结构生成优化的CPUGPU代码 对特定硬件做针对性的优化。 自动调整，在特定硬件优化中用于参数调整的搜索空间巨大（比如对张量、数据进行切分，数据如何排布），需要利用自动调整来确定最佳参数设置。 利用 HalideTVM 使用的方式，利用机器学习去做搜索优化。其允许调度和计算表达分开。 利用多面体模型 poly model进行参数调整 优化内核库，厂商特定的优化内核库。因为经过了一些特定的优化，可能会比编译器生成的算子效率更高。 AI编译器中比较难解决的问题 动态shape问题 现阶段主流的AI编译器主要针对特定的静态shape输入完成编译优化，对于包含控制流的动态计算图提供的支持有限 在NLP任务中，输入序列的长度是不固定的，会存在大量的动态shape需求。这时候需要前端将计算图改写为静态计算图，或者只优化适合编译器的部分。 部分任务很难通过人工来改写成静态化计算图。 Python编译静态化问题，将动态图转为静态图目前来说大部分是通过torch.jit.trace()去做的，导出onnx就是在做torch.jit.trace()。 类型推导：有python动态类型转换到编译器的静态类型，遇到的问题就是静态类型不一定能够表达所有的动态类型。 控制流的表达，if、else、while、for等。在torch.jit.trace()时候的一个条件就是不能有类似的控制流。 JIT的编译性能，会增加额外的编译开销，有编译肯定会损失性能。 就比如我之前在复旦微的ai芯片上做的一个项目，pytorch中定义的某些算子，在导出静态图的时候，需要做替换，用静态去表达动态，这个时候就会出现性能损失。 如何通过AI编译器压榨硬件性能，特别是DSA类芯片。编译器在性能优化的难度和复杂度上的难度变大。 难度，性能优化依赖图算融合优化。独立优化无法充分发挥芯片性能，需要例如子图切分、子图内垂直融合优化和水平并行加速等。 复杂度随着硬件的复杂度提升而提升，标量+向量+张量+加速指令、多级的存储结构，导致 kernel 实现 schedule、tilling、vectorization、tensorization 复杂 处理神经网络特性：自动微分、自动并行等 自动并行，大模型训练遇到内存墙性能墙等问题需要并行策略来解决。 Scale out(多维混合并行能力，数据、张量、流水线并行)，Scale up（重计算、混合精度、异构并行）。二者最大的挑战就是效率墙。 面向HPC场景自动微分的要求更高。 如何平衡易用性和性能 question 图算能否统一表达，统一编译优化，形成更通用的AI编译器？ 完全的自动并行是否可行？ AI芯片需要编译器么？需要AI编译器么？ future 编译器分开推理和训练，AOT和JIT两种编译方式并存 IR形态，需要有类似MLIR对AI统一的IR表达 自动并行、自动微分 kernel 自动生成，降低开发门槛，快速的实现高效和高泛化的算子","tags":["AI Sys","编译器"],"categories":["AI Sys"]},{"title":"LLVM 学习记录 0x","path":"/2025/09/24/LLVM学习02/","content":"LLVM 学习记录 0x, 跟随大佬的脚步 LLVM整体设计LLVM VS GCC LLVM使用C++语言开发，是基于模块化，可扩展的设计，将编译过程分为了多个阶段，提供了 LLVM IR 作为通用的数据结构进行代码优化和生成。同时LLVM的设计可以使优化在编译过程的各个阶段进行，实现了细粒度的优化。 若要开发一种新的编程语言，只需要实现一个新的编译前端，已有的优化和后端可以实现复用。 GCC使用C语言开发，集成了多个前端和后端的传统编译器，设计更加紧密。是耦合的，一个前端对应一个优化器对应一个后端。 LLVM结构 前端，解析代码，检查语法错误，生成抽象语法树AST，再生成中间表示，LLVM IR。有多种解析器，CC++使用clang解析，fortran使用flang解析。 优化器，对代码进行优化，例如消除冗余计算，同时有很多优化选项可选。 后端，代码生成器，将 LLVM IR 生成汇编或二进制文件。一对多，可对应多个后端。可针对新的后端平台进行扩展。 项目组成 Clang，用于解析CC++代码 MLIR，构建可重用和可扩展编译器基础设施的新颖方法 OpenMP，提供了一个OpenMP运行时库函数 polly，使用多面体模型实现了一套缓存局部性优化以及自动并行和向量化 LLDB、libc++、lib++ ABI、compiler-rt、libclc、klee、LLD、BOLT 命令工具图片为LLVM的处理流程，截自于文章顶部B站视频。 llc 是 LLVM 的静态编译器 lli 可从.ll或.bc执行程序 llvm-as LLVM汇编器 llvm-dis LLVM反汇编器 opt LLVM优化器 大致流程如下，源代码通过clang解析，生成 LLVM IR代码，即.ll文件（不加 -S会编译成 bc文件）。.ll文件可使用llvm-as转换为bc文件，也可使用llvm-dis转换为.ll文件。可使用lli直接执行，可以使用llc转换为汇编文件。最后生成可执行文件。 # 解析源代码，生成 LLVM IR 代码clang -S -emit-llvm main.c -o main.ll# .ll 和 .bc 互转llvm-as main.ll -o main.bcllvm-dis main.bc -o main.lllli main.bc # 或者lli main.bc，直接执行llc main.bc -filetype=asm -o main.s # 生成汇编文件clang main.bc -o main # 生成可执行文件，也可使用gcc./main LLVM前端-clang基本概念clang项目为LLVM项目提供了C语言系列（c、c++、opencl、cuda等）的语言前端以及工具基础设施。项目中包括了clang driver以及clang前端。clang driver做的事情就比如上述代码中生成可执行文件，就是超出了前端的范围的事情就是clang driver做的。clang前端就只包括了语法分析，生成AST以及LLVM IR。 功能分析这里只分析clang前端的几项功能 预处理（preprocessor）：头文件以及宏的处理 词法分析（lexer）：词法分析器，从左至右逐行扫描源代码中的字符，识别字符以及确定类型，转换为 token 语法分析（parser）：从 token 中识别各类短语，不考虑代码在干什么，只考虑代码是否写正确，物理意义是否写错，并构造语法分析树AST 语义分析（sema）：遍历AST收集属性信息以及进行语义检查，就是在考虑代码在干什么，会检查代码是否违背高级语言的语法。 代码生成（codegen）：将AST转换为相应的LLVM IR代码 AST结构# 查看ASTclang -Xclang -ast-dump -fsyntax-only test.c declaration (Decl)，声明 statement (Stmt)， 指令 expression (Expr)， 表达式 LLVM IR基本概念LLVM IR基本是前后端无关的，图片取自ZOIM大佬。LLVM IR由两个基本原则 LLVM IR遵循 SSA，静态单赋值，我的另一篇文章中也提到了，不能对同一个变量二次赋值。这个原则使得LLVM IR代码组织为三地址指令序列以及提出了虚拟寄存器概念来执行快速优化。 三地址指令：特性是每个三地址指令都可以被分解为一个四元组（4-tuple）的形式，(op, x1, x2, z)，每条指令最多有三个操作数。 整个程序的IR存储到磁盘让链接时优化易于实现。 示例 void test(int a, int b) int c = a*b+100; 可使用如下命令将上述代码转化为 LLVM IR clang -S -emit-llvm test.c LLVM IR表示LLVM IR有三种形式 内存中的形式，编译中间语言，无法通过文件的形式得到的指令类。 bitcode形式，是一种序列化的二进制表示形式。就是在硬盘上存储的二进制语言，就是.bc文件。 LLVM 汇编文件形式，也是一种序列化的表示形式，与bitcode的区别就是可读。就是.ll文件。示例LLVM IR 内存模型 Module，一个.ll文件就是一个Module类，是LLVM IR的基本单位 Function，除去属性说明一类的，剩下的函数单元。 BasicBlock，基本代码块，没有控制流逻辑的流程，相当于程序流程图的基本过程。一个function中至少有一个BasicBlock Instruction，指令类就是 LLVM IR中的基本操作。BasicBlock中有若干个Instruction，并且都以termination instruction结尾。可生成控制流图CFG，添加-only就是一个简图，根据BasicBlock的名字做的。去掉-only可显示基本块中的指令opt - analyze -dot-cfg-only test.llopt - analyze -dot-cfg test.lldot -Tpng xxx.dot -o 1.png LLVM IR内存模型中最重要的概念：Value、Use、User，有了整套的数据结构可以很方便的操作use-def和def-use这个链条。 中间优化遍可以去看官方文档，去看每一个pass是怎么做的 Analysis Passes, 分析类优化遍，负责发掘性能和优化机会，不做具体优化工作 Transform Passes，转换类优化遍，做具体的优化工作，生成必需的数据结构-IR， Utility Passes，使用类优化遍pass管理器，就是将pass排了个序。在分析pass和转化pass之间有两种依赖类型 显示依赖：转换pass需要依赖一种分析pass，那么pass管理器会安排它所依赖的分析pass在它之前运行。 隐式依赖：转换或分析pass要求IR代码运用特定表达式，则需手动的以正确的顺序把这个pass加入pass队列中Pass API，pass类是实现优化的主要资源，但我们是通过子类来使用pass类。分层次分力度的使用。常见子类如下： ModulePass FunctionPass BasicBlockPass# 这个命令可以打印出运行了哪些passopt test.bc -instcount -time-passes -domtree -o test-tmp.bc -stats LLVM后端-代码生成(codegen)基本概念LLVM 有目标无关代码生成器的框架，提供了一套可重用的组件，用于LLVM IR转换为指定目标的机器代码。LLVM后端的主要功能就是代码生成，包括若干个代码生成分析转化pass将LLVM IR转换成目标架构的机器代码源码结构： libTarget: 特定目标的抽象目标描述接口的实现。 libCodeGen: 用于实现本机代码生成的各个阶段（寄存器分配，调度、堆栈帧表示等）的目标无关算法。 libExecutionEngineJIT: 目标独立的JIT组件 整个流水线图片取自ZOMI大佬 指令选择（最重要，添加新后端时大部分工作量都集中在这部分）：将LLVM IR代码转换成目标指令的DAG（有向无环图），分解成一个一个的节点，每个DAG能够表示单一基本块的计算。节点表示指令，而边编码了之零件的数据流依赖。目标就是让LLVM代码生成库能够运用基于树的模式匹配指令选择算法。 通过这种模式匹配将节点匹配到机器指令，指令的描述和模式由LLVM后端开发者编写，主要使用TableGen语法在td文件中描述，复杂模式也可通过c++编码实现。 指令调度和格式化：将DAG转换成指令序列，可设置符合目标硬件的调度策略。第一次指令调度，也被称为前寄存器(RA)分配调度。同时对指令排序，尝试发现尽可能多的指令层次的并行，然后将指令变换为 MachineInstr 三地址表示。 基于SSA的机器代码优化：（可选）一系列基于SSA机器码优化，比如模块调度，写在CodeGen中 指令调度：第二次指令调度，也被称为后寄存器分配调度，此时可获取真实的寄存器信息，某些类型寄存器存在延迟，它们可被用以改进指令顺序。 寄存器分配：把LLVM IR中无限个虚拟寄存器映射到真实寄存器中，寄存器不够时先缓存到内存中。 PrologEpilog代码插入：对之前整体代码做优化。 后期机器代码优化：对MachineInstr进行的优化 生成代码：将指令从 MachineInstr 表示变换为MCInst实例，新的表示更适合汇编器和链接器，可以输出汇编代码或者输出特定目标格式的二进制代码。 指令选择指令选择方法 基于SelectionDAG的指令选择，默认方法，通过IR的一层一层降级实现 LLVM IR - - - SelectionDAG - - - MachineDAG - - - MachineInstr - - - MCInst 全局指令选择，相对于第一个更通用，没有那么复杂，支持后端不多。同时因为通用，效果相对来说不好。后端都特定了，确实感觉没必要再通用这个了。 综合基于SelectionDAG的指令选择和快速指令选择二者的有点，不引入新的DAG表示，在函数级别进行指令选择，发掘出更多的全局优化机会（使用时加-global-isel选项） LLM IR - - - G-MachineInstr - - - MachineInstr - - - MCinst 寄存器分配方式 Basic: 一种增量的寄存器分配方法。 Fast: 调试构建的默认配置。 Greedy（贪心）: 默认分配器, 是 Basic 分配器的一个高度调优实现，包含了全局活动范围分割。 PBQP: 基于分区布尔二次规划的寄存器分配器。 基于SelectionDAG的指令选择的步骤（重点） 初始化DAG：从 LLVM IR简单转换到SelectionDAG 优化SelectionDAG：对SelectionDAG进行简单的优化，类似合并，删除冗余操作等优化，使后续阶段更简单。 合法化SelectionDAG类型：比如检查数据类型，后端可能会存在不支持操作的数据类型，比如复旦微的ai芯片，只支持i8,i16。f16 f32 这种数据类型就要程序员进行处理，是不处理还是用其他方法表示。 优化SelectionDAG：消除因为类型合法化造成的代码冗余。 合法化SelectionDAG操作：检查数据的操作，后端可能会存在不支持的操作，比如后端只支持整型的加减，不支持浮点的加减，这时候就需要检查，是拼接现有操作以扩充支持还是不处理。 优化SelectionDAG：消除操作合法化造成的代码冗余 从DAG中选择指令：将DAG操作与Target指令相匹配。将目标无关的DAG转换为Target指令的另一个DAG。 SelectionDAG调度：为选择出的DAG指令分配一个线性顺序，将其发送至正在编译的MachineFunction中。可以用llc进行调试，可打印出DAG。可以检查编译过程中是否出现逻辑性的错误。 编写后端 需要程序员来创建TargetMachine类的子类来描述目标机器的特性。 TargetMachine 是LLVM用来表示目标架构整体属性的类，这个类告诉编译器我要为哪个后端生成代码。信息包括目标架构、大小端、指令对齐、调度模型、默认ABI、浮点向量支持等，把这个类完整的写好，后续pass，寄存器分配等步骤才能用上正确的参数。 描述目标平台的寄存器集，用TableGen从一个特定目标平台的RegisterInfo.td输入文件来生成寄存器定义，寄存器别名，寄存器类，要为TargetRegisterInfo类写一些额外的代码。 寄存器的定义决定了编译器能否知道由哪些物理寄存器，并生成对应的数据结构共对应的寄存器分配器、指令选择、寄存器检查类使用。 描述目标平台的指令集。 需要把机器指令，比如加减、加载、存储、移位等操作在TableGen中定义为指令Instruction。 描述从指令的DAG表示到本地目标机器平台的指令的LLVM IR的选择与优化。 LLVM IR在代码生成阶段会被转换为SelectionDAG，后端需要把DAG中的抽象节点翻译成目标指令，并且针对目标平台做一些优化。 编写将LLVM IR转为一个GAS（GNU Assembler）格式的汇编打印器。","tags":["编译器","LLVM"],"categories":["LLVM"]},{"title":"LLVM 学习记录 0x","path":"/2025/09/24/LLVM学习01/","content":"LLVM IR 学习记录 0x, 跟随大佬的脚步，感觉上像是定义的一种特殊的数据结构。LLVM IR类似于精简指令集RISC，支持简单指令的线性序列。 数据表示数据区与符号表 寄存器中的数据 内存中的数据 栈上的数据 数据区的数据, 能够在程序的任一位置使用，这类全局静态变量应越少越好。LLVM IR 中定义一个存储在数据区的全局变量，示例如下，定义一个 i32 的全局变量，并初始化为0。在 LLVM IR中，全局变量以@开头，局部变量以%开头，同样也是临时寄存器（虚拟寄存器）。 ;分号是注释@global_variable = global i32 0 只读的全局变量，即常量，可用 constant 代替 global：@global_constant = constant i32 0 符号与符号表clang main.ll -o main nm main 用 nm 命令查看程序的符号表 整体的符号处理过程 编译器对源代码按文件进行编译，对于文件中的未知函数，用一个符号代替。对于文件中实现的函数，也用符号代替并将其暴露。 连接器收集所有的目标文件，对于每个文件而言，将其记录下的位置函数符号对比，若匹配则成功解析。 部分符号在程序加载、执行时由动态连接库给出，动态连接器将进行这些符号的解析。 LLVM IR 中为解决函数名与其余第三方库的函数名重复问题，提出连接与可见性的概念，提供了 Linkage Type 以及 Visibility Styles 两个修饰符来控制相应的行为。 链接类型 什么都不加，默认为 externeal 把全局变量的名字放在符号表中，这个函数就可以在链接时被其余编译单元看到。 private @global_variable = private global i32 0 这样写的变量，用 nm 查看符号表是不会出现该变量的 internal @global_varibale = internal global i32 0 这样写就是全局变量，但是是局部符号，类似与C中的static关键词。用 nm 查看符号表，可以看到，但不会参与符号解析。 可见性 实际使用中比较少，主要分为三种，如下所示。主要区别在于符号能否被重载 default，可以被重载 hidden，不将变量放在符号表中，其余模块不可直接引用这个符号 protected，不可被重载 可抢占性 dso_local ，在 LLVM 中被称作运行时抢占性修饰符。 示例C语言 int a;extern int b;static int c;void d(void);void e(void) static void f(void) 使用clang编译成 LLVM IR后如下 @a = dso_local global i32 0, align 4 ; align 表示字节对齐@b = external global i32, align 4@c = internal global i32 0, align 4declare void @d()define dso_local void @e() ret voiddefine internal void @f() ret void 通过上述的示例可以得出， C语言中的static，也就是当前文件中定义，别的文件不可以用的，都会加上internal修饰符 C语言中的extern，也就是别的文件中定义的，全局变量会加上external修饰符，函数会使用declare C语言中定义的，可以给别的文件使用的全局变量或函数，不会加上链接类型修饰符，并且会加上dso_local保证不会被抢占 寄存器和栈加减乘除，比大小等大多数的数据操作都需要操作寄存器中的数据，那为什么需要把数据放在栈上呢。原因如下 寄存器数量不够，变量数量多于寄存器数量。 需要操作内存地址，因为寄存器没有通用的地址表示，所以需要把数据放在栈上来完成对地址的操作。（？寄存器应该可指定地址的吧，这边不是很理解。）在不操作内存地址的前提下，栈用来扩充寄存器数量。如下是一个C程序// max.cint max(int a, int b) if (a b) return a; else return b; int main() int a = max(1, 2); return 0; 汇编文件为movl $1, %edimovl $2, %esicallq max 在不使用llvm进行优化的前提下，会将参数1和2先放到两个寄存器中，然后再存入栈，再将其中一个存入第三个寄存器中，再比较第三个寄存器的值与另一个栈上变量的值，将较大的值存入另一个栈上，再将这个栈上的数据拷贝至寄存器中。若使用llvm优化呢？clang -o1 -S max.c汇编代码为movl %esi, %eaxcmpl %esi, %edicmovgl %edi, %eaxretq 此时所有的操作，全部都在寄存器上了。 寄存器通过上述的例子，可知若寄存器数量足够，且不需要操作内存地址，寄存器是更加高效的。所以LLVM IR引入了虚拟寄存器的概念, 以 % 开头%local_variable = add i32 1, 2LLVM IR会帮我们把变量放在寄存器中，若寄存器用完了，会将其放在栈上，但对于我们而言，实际上都是虚拟寄存器。LLVM IR对寄存器的使用粗略理解如下： 当所需要的寄存器数量较少时，使用 caller-saved register，不需要保留的寄存器。 当 caller-saved register数量不够时，将 caller-saved register的值压至栈中，然后使用callee-saved register. 当寄存器全部用完后，剩余的虚拟寄存器实际上就是使用栈了。 栈LLVM IR的虚拟虚拟寄存器可保证我们在不需要操作地址时，无限的使用寄存器。但涉及到地址操作以及可变的变量时，仍然需要使用栈。LLVM IR直接使用alloca即可, alloca就是在函数栈帧中分配内存 %local_variable = alloca i32 数据的使用LLVM IR把全局变量和栈上变量全部视为指针, 需要使用load（读取）和store（写入）来操作这些值 ; 全局变量@global_variable = global i32 0; 局部变量%local_variable = alloca i32 0; 在 LLVM IR中全部视为指针;%1 = add i32 1, @global_variable; 所以这样操作是错的; 把global_variable的值赋值给寄存器%1%1 = load i32, ptr @global_variable%2 = add i32 1, %1; 将1存到全局变量中store i32 1, ptr @global_variable LLVM IR严格遵守SSA(Static Single Assignment，静态单次分配)，每个变量只能被赋值一次。 这样就可以使得编译器找到反向追溯到定义的唯一指令。 其次SSA形式会定义一个user-define链条，这个链条可以方便的去追溯每个值对应的指令是怎么使用的。%1 = load i32, ptr @global_variabl%1 = load i32, ptr @local_variable; 这样是不被允许的 类型系统汇编语言是弱类型的，但LLVM IR是强类型的语言。有利于维护以及优化。 基本数据类型 空类型（void） 整型（iN）,这里的N可以是任意正整数，i1, i2, i4, i8, i10086,常用的就是i1以及8个整数倍，i1即bool类型，只有两个值，true和false。示例如下 %boolean_variable = alloca i1store i1 true, ptr %boolean_variable%integer_variable = alloca i32store i32 128, ptr %integer_variablestore i32 -128, ptr %integer_variable 浮点型（float, double） 符号在LLVM IR中，整型默认是有符号类型，可以直接将-128以补码的形式赋给i32类型的变量。整型的有无符号体现在操作指令而不是类型上。对于两个整型的除法，LLVM IR提供了udiv和sdiv指令分别适用无符号整型除法和有符号整型除法。示例如下 ;main.ll;目标平台target triple = x86_64-unknown-linux-gnudefine i4 @main() %1 = udiv i4 -6, 2 ; Get (16-6) / 2 = 5 %2 = sdiv i4 -6, 2 ; Get (-6) /2 = -3 ret i4 %1 转换指令将i8类型转为i32类型，将i32转为i8需要特定的转换指令。LLVM IR提供三种指令 trunc .. to 长的整型转换为短的整型，是直接把多余的高位去掉。例如 %trunc_integer = trunc i32 257 to i8 ; Trunc 32 bit 100000001 to 8 bit, get 1 短整型转长整型较复杂。因为在补码中最高位是符号位。并不表示实际的数值。有时需要i8类型的-1扩展到i32时仍然时-1,所以不能只有在更高位补0的操作。 zext .. to，零扩展，即在更高位补0 sext .. to，符号扩展，用原数的符号位来填充示例如下%zext_integer = zext i8 -1 to i32 ; Extend 8 bit 0xFF to 32 bit 0x000000FF, get 255%sext_integer = sext i8 -1 to i32 ; Extend 8 bit 0xFF to 32 bit 0xFFFFFFFF, get -1 LLVM IR也提供了整型与浮点型之间的转换指令，但是需要注意，当大数转为小数时，并不保证截断。例如196.7转为i8时，会超过i8能表示的范围，会产生为定义行为。所以浮点型和整型在互相转换时需要调整。 fptoui .. to，浮点数- - -无符号整型 fptosi .. to，浮点数- - -有符号整型 uitofp .. to，无符号整型- - -浮点数 sitofp .. to，有符号整型- - -浮点数 指针类型LLVM IR中指针类型即ptr，类似于C语言中的void *。LLVM IR提供了指针与整型之间转换的指令 ptrtoint .. to inttoptr .. to%x = alloca i32 ; %x is of type ptr, which is the address of variable x%y = alloca i32 ; %y is of type ptr, which is the address of variable y%address_of_x = ptrtoint ptr %x to i64%address_of_y = sub i64 %address_of_x, 4%also_y = inttoptr i64 %address_of_y to ptr ; %also_y is of type ptr, which is the address of variable y 聚合类型数组 %a = alloca [4 x i32] ; 相当于 int a[4]@global_array = global [4 x i32] [i32 0, i32 1, i32 2, i32 3] ; 初始化@global_string = global [12 x i8] cHello world\\00 ; 字符数组 结构体 %MyStruct = type i32, i8; 初始化结构体@global_structure = global %MyStruct i32 1, i8 0 ; or@global_structure = global i32, i8 i32 1, i8 0 在操作聚合类型的某些字段时，需注意区分聚合类型是指针形式，还是寄存器形式。 指针形式的聚合类型访问数组元素字段，核心的指令就是getelementptr，需要传入四个参数，第一个参数是要取地址的指针，第二个参数是要操作的指针，第三个参数是取偏移量为几的元素，第四个参数是对于获取到的元素，取索引为几的字段。 %MyStruct = type i32, i32 ; 定义了一个结构体类型define void @foo(ptr %my_structs_ptr) %my_y_in_stack = alloca i32 ; 定义一个i32类型的局部变量，%my_y_in_stack是其地址 %my_y_ptr = getelementptr %MyStruct, ptr %my_structs_ptr, i64 2, i32 1 ; getelementptr用于获取结构体中某个成员的地址 %my_y_val = load i32, ptr %my_y_ptr ; 将%my_y_ptr的值加载到%my_y_val store i32 %my_y_val, ptr %my_y_in_stack ; 将%my_y_val的值存入%my_y_in_stack ret void 访问指针字段，与访问数组字段相似，只是将偏移量修改为了0。LLVM IR中将所有的指针全部看作一个指向数组首地址的指针。 %MyStruct = type i32, i32 define void @foo(ptr %my_structs_ptr) %my_y_in_stack = alloca i32 %my_y_ptr = getelementptr %MyStruct, ptr %my_structs_ptr, i64 0, i32 1 %my_y_val = load i32, ptr %my_y_ptr store i32 %my_y_val, ptr %my_y_in_stack ret void 级联访问，getelementptr指令还可以接收多个参数。下面代码中，getelementptr的后两个参数含义为取索引为2的字段中索引为4的元素。 %MyStruct = type i32, [5 x i32]%my_structs = alloca [4 x %MyStruct]%1 = getelementptr %MyStruct, ptr %my_structs, i64 2, i32 1, i64 3 值形式的聚合类型LLVM IR提供了 extractvalue 和 insertvalue 指令来操作存储值的聚合类型。 extractvalue，用来取值 insertvalue，用来赋值; extract_insert_value.ll%MyStruct = type i32, i32@my_struct = global %MyStruct i32 1, i32 2 define i32 @main() %1 = load %MyStruct, ptr @my_struct %2 = extractvalue %MyStruct %1, 1 ; 获取%1第二个字段的值 %3 = insertvalue %MyStruct %1, i32 233, 1 ; 将%1第二个字段赋值为i32 233， %3就会是给%1赋值之后的值 ret i32 0 元数据类型LLVM IR中，以!开头的标识符为元数据。元数据是为了将额外的信息附加在程序中传递给LLVM后端，使后端能够好地优化或生成代码。用于Debug的信息就是通过元数据形式传递的。我们可以使用-g选项 clang -S -emit-llvm -g test.c 属性define void @foo() nounwind ; ...attributes #0 = noinline nounwind optnone ssp uwtable correctly-rounded-divide-sqrt-fp-math=false darwin-stkchk-strong-link disable-tail-calls=false frame-pointer=all less-precise-fpmad=false min-legal-vector-width=0 no-infs-fp-math=false no-jump-tables=false no-nans-fp-math=false no-signed-zeros-fp-math=false no-trapping-math=false probe-stack=___chkstk_darwin stack-protector-buffer-size=8 target-cpu=penryn target-features=+cx16,+cx8,+fxsr,+mmx,+sahf,+sse,+sse2,+sse3,+sse4.1,+ssse3,+x87 unsafe-fp-math=false use-soft-float=false 代码中，nounwind就是一个属性。LLVM IR引入了属性组的概念，以 # 开头。","tags":["编译器","LLVM"],"categories":["LLVM"]},{"title":"leetcode刷题记录-20250811，day20，二叉树","path":"/2025/08/26/leetcode刷题记录-20250811，day20，二叉树/","content":"LeetCode 刷题记录 - 2025-08-11今天刷题总结如下： 题目 1: LeetCode 235. 二叉搜索树的最近公共祖先 题目描述：给定一个二叉搜索树, 找到该树中两个指定节点的最近公共祖先。 百度百科中最近公共祖先的定义为：“对于有根树 T 的两个结点 p、q，最近公共祖先表示为一个结点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。”难度: Medium 思路二叉树题目一般考虑递归。对于递归，1、确定递归函数的参数和返回值；2、确定终止条件；3、确定单层递归的逻辑。这道题是在二叉搜索树中找到两个节点的最近公共祖先，首先考虑了一下中序遍历发现没什么用。其次考虑到了二叉搜索树的特点，就发现查找的规律，1、若两个节点一个大于根节点一个小于根节点则最近公共祖先就是这个根节点。2、若两个节点中某个节点等于当前根节点，则最近公共祖先就是这个根节点3、若两个节点都大于或都小于根节点，则去相应的右子树或左子树查找。💡 OS： 注意题目隐藏的规律，找到规律就比较简单，否则就等于在二叉树里找两个节点的最近公共祖先了。 代码如下class Solution public: TreeNode* lowestCommonAncestor(TreeNode* root, TreeNode* p, TreeNode* q) if (!root) return nullptr; if (p-val == root-val || q-val == root-val ) return root; if ((p-val root-val q-val root-val) || (p-val root-val q-val root-val)) return root; if (p-val root-val q-val root-val) return lowestCommonAncestor(root-right, p, q); if (p-val root-val q-val root-val) return lowestCommonAncestor(root-left, p, q); return nullptr; ; 题目 2: LeetCode 710. 二叉搜索树中的插入操作 题目描述：给定二叉搜索树（BST）的根节点 root 和要插入树中的值 value ，将值插入二叉搜索树。 返回插入后二叉搜索树的根节点。 输入数据 保证 ，新值和原始二叉搜索树中的任意节点值都不同。 注意，可能存在多种有效的插入方式，只要树在插入后仍保持为二叉搜索树即可。 你可以返回 任意有效的结果 。难度: Medium 思路二叉树题目一般考虑递归。对于递归，1、确定递归函数的参数和返回值；2、确定终止条件；3、确定单层递归的逻辑。这道题是在二叉搜索树中插入节点，也是比较简单的吧，就一直遍历二叉搜索树就好了，找到某个节点没有左或右孩子，且新值小于或大于这个节点，就把新节点插入进去就好了。💡 OS： 没啥可注意的，就正常前序遍历二叉搜索树就可以了，条件不要写错。 代码如下class Solution public: void travelTree(TreeNode* root, int val) if (!root) return; TreeNode* temp = new TreeNode(val); if (!root-left root-val val) root-left = temp; if (!root-right root-val val) root-right = temp; if (root-left val root-val) travelTree(root-left, val); if (root-right val root-val) travelTree(root-right, val); return; TreeNode* insertIntoBST(TreeNode* root, int val) if (!root) return new TreeNode(val); travelTree(root, val); return root; ; 题目 3: LeetCode 450. 删除二叉搜索树中的节点 题目描述：给定一个二叉搜索树的根节点 root 和一个值 key，删除二叉搜索树中的 key 对应的节点，并保证二叉搜索树的性质不变。返回二叉搜索树（有可能被更新）的根节点的引用。 一般来说，删除节点可分为两个步骤：首先找到需要删除的节点；如果找到了，删除它。难度: Medium 思路二叉树题目一般考虑递归。对于递归，1、确定递归函数的参数和返回值；2、确定终止条件；3、确定单层递归的逻辑。这道题是在二叉搜索树中插入节点，也是比较简单的吧，就一直遍历二叉搜索树就好了，找到某个节点没有左或右孩子，且新值小于或大于这个节点，就把新节点插入进去就好了。💡 OS： 没啥可注意的，就正常前序遍历二叉搜索树就可以了，条件不要写错。 代码如下","tags":["刷题","数据结构","算法","二叉树"],"categories":["LeetCode"]},{"title":"Versal ACAP学习记录","path":"/2025/08/25/Versal-ACAP学习记录/","content":"最近需要把Versal ACAP重新学起来（(ㄒoㄒ)~~），在断断续续的学习过程中发现，中文教程很少，相关资料也比较少，大多只能按照xilinx 的文档来做。有时候遇到一些bug需要排查很久才知道是哪的问题（即使是很简单的问题），对这块板子的认识比较浅薄。所以记录一下使用过程中的问题，以及整理成文档方便以后还能记起来。。从官方文档的lenet开始，到minist结束吧。主要是网络的部署，其他的用到再说吧。","tags":["Versal ACAP","软硬件协同设计"],"categories":["Versal ACAP"]},{"title":"AI Sys学习记录 --- AI 计算体系概述","path":"/2025/08/22/AI Sys学习记录01/","content":"前言最近在学习zomi酱大佬的AI Sys，记录一下作为菜鸡的学习历程，也分享一下自己浅薄的认识(狗头保命)。不会严格按照大佬的章节安排走，也不是一节一个记录，可能看过去一部分才回头来写之前东西。个人见解，个人认识，有不妥或错误之处可以指出，欢迎交流学习。 1. 并行计算并行计算是把一个计算任务拆成若干可同时执行的子任务，在多个处理单元（多核CPU、GPU、加速器、集群节点等）上同时运行，从而缩短总耗时、提高吞吐、或者节能。和”并发”不同：并发是同一时间段内交替推进多任务；并行是同一时刻真正同时做多件事（需要多个硬件执行单元）。 并行性的来源，如何拆分任务？数据并行（Data Parallelism）任务并行（TaskFunctional Parallelism）向量化（SIMDSIMT）子任务越小并行度越高，但同步调度成本也更高，要权衡速度与能耗。 并行模式生产-消费模式一个或多个生产者负责不断生成数据任务。一个或多个消费者负责不断取数据处理任务。二者通过共享的缓冲区交互（队列），生产者放入，消费者取出。生产和消费可并行，互不等待。例如，前段时间刚做过的RK3588开发板上部署模型（后续会更新），为加速计算，开线程方式就是使用了这种模式。实际应用中从摄像头采集帧，丢到队列中，同时充分利用NPU三个核心，（生产线程）。然后从队列中取帧，前处理，丢到NPU进行推理，后处理（消费线程）。可极大提升 NPU 利用率，提高帧率。由单线程的5FPS左右提高到18~19FPS。Fork-JoinFork：将任务拆分为多个可并行的子任务。Join：等待子任务完成之后，合并结果，继续执行后续任务。例如，模型分布式训练中的数据并行训练，多块 GPU 同时计算梯度，最后All-Reduce 汇总。流水线并行将任务分成若干阶段，向流水线一样流动执行。例如，在复旦微电子100TAI上做过的模型部署工作（后续会更新）。将部署任务划分成前处理，推理，后处理，三个阶段，开线程方式使用这种方式，同时使用多个缓存区域缓存数据。这样做理论上极限速度就是推理速度，可充分利用27T的算力。但是芯片上只有一个ai core，不能像RK3588那样利用三个NPU核心分别处理三帧图像。 2. Roofline模型3. 数据位宽","tags":["AI Sys","学习记录","硬件体系结构","编译与计算架构","推理系统&引擎","AI 框架"],"categories":["AI Sys"]},{"title":"leetcode刷题记录-20250809，day18，二叉树","path":"/2025/08/22/leetcode刷题记录-20250809，day18，二叉树/","content":"LeetCode 刷题记录 - 2025-08-09今天刷题总结如下： 题目 1: LeetCode 530.二叉搜索树的最小绝对差 题目描述：给你一个二叉搜索树的根节点 root ，返回 树中任意两不同节点值之间的最小差值 。 差值是一个正数，其数值等于两值之差的绝对值。难度: Easy 思路二叉树题目一般考虑递归。对于递归，1、确定递归函数的参数和返回值；2、确定终止条件；3、确定单层递归的逻辑。一开始的时候也没有考虑到中序遍历二叉搜索树是递增序列，，，后来也是看了思路之后才发现中序遍历是递增的，清楚这一点之后这道题就不难了。还有就是要维护一个全局的前值，这样就可以比较当前元素与前一个元素的差值了。💡 OS： 注意中序遍历二叉搜索树就是一个递增的遍历，最小绝对差出现在相邻元素之间，即可只比较相邻元素直接的差值。维护全局的前值。 代码如下class Solution public:int result = INT_MAX;int prev_val = -1; // 中序遍历二叉搜索树就是一个递增去遍历 // 最小绝对差出现在相邻元素之间，即可只比较相邻元素直接的差值 void inorder(TreeNode* root) if (!root) return; inorder(root-left); if (prev_val!=-1) result = min(result, root-val-prev_val); prev_val = root-val; inorder(root-right); int getMinimumDifference(TreeNode* root) inorder(root); return result; ; 题目 2: LeetCode 501.二叉搜索树中的众数 题目描述：给你一个含重复值的二叉搜索树（BST）的根节点 root ，找出并返回 BST 中的所有 众数（即，出现频率最高的元素）。 如果树中有不止一个众数，可以按 任意顺序 返回。 假定 BST 满足如下定义：结点左子树中所含节点的值 小于等于 当前节点的值结点右子树中所含节点的值 大于等于 当前节点的值左子树和右子树都是二叉搜索树难度: Easy 思路二叉树题目一般考虑递归。对于递归，1、确定递归函数的参数和返回值；2、确定终止条件；3、确定单层递归的逻辑。这个题是去找二叉搜索树的众数，对于二叉搜索树的题，包先想到中序遍历，然后发现这个题用不用都行。。。其次我又想到之前做的哈希表的题，用哈希表去记录这种出现次数很方便，所以我维护了一个全局的哈希表去记录每个数出现的次数，最后遍历一下把数放进vector中就好了。 代码如下class Solution public:unordered_mapint, int record; static bool cmp_value(const pairint, int left,const pairint,int right) return left.second right.second; void travelTree(TreeNode* root) if (!root) return; travelTree(root-left); record[root-val]++; travelTree(root-right); vectorint findMode(TreeNode* root) vectorint result; if (!root-left !root-right) return root-val; travelTree(root); auto max_i = max_element(record.begin(), record.end(), cmp_value); for (auto i : record) if (i.second == max_i-second) result.push_back(i.first); return result; ; 题目 3: LeetCode 236. 二叉树的最近公共祖先 题目描述：给定一个二叉树, 找到该树中两个指定节点的最近公共祖先。 百度百科 中最近公共祖先的定义为：“对于有根树 T 的两个节点 p、q，最近公共祖先表示为一个节点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。”难度: Medium 思路二叉树题目一般考虑递归。对于递归，1、确定递归函数的参数和返回值；2、确定终止条件；3、确定单层递归的逻辑。这道题要找给定两个节点的最近的公共祖先，首先考虑要遍历二叉树找到该节点，把路径保存下来。这样做要遍历两边二叉树，然后两个for循环找公共节点，然后返回深度最大的节点。写代码时遇到的问题就是我一开始没有给停止信号，定义的void函数，递归条件哪里就有问题，当找到节点就返回。然后提交就发现只能通过一个或者直接溢出了。。后来定义bool函数，找到节点返回true就不递归了，否则返回false。这样做就可以了。 💡 OS： 这里要注意回溯，当左右子树都没找到节点时pop一个节点。 代码如下class Solution public: bool travelTree(TreeNode* root, TreeNode* node, vectorTreeNode* record) if (!root) return false; record.push_back(root); if (root == node) return true; if (travelTree(root-left, node, record)) return true; if (travelTree(root-right, node, record)) return true; record.pop_back(); return false; TreeNode* lowestCommonAncestor(TreeNode* root, TreeNode* p, TreeNode* q) vectorTreeNode* an_p; vectorTreeNode* an_q; travelTree(root, p, an_p); travelTree(root, q, an_q); vectorTreeNode* result; for (int i = 0; i an_p.size(); i++) for (int j = 0; j an_q.size(); j++) if (an_p[i] == an_q[j]) result.insert(result.begin(), an_p[i]); return result[0]; ;","tags":["刷题","数据结构","算法","二叉树"],"categories":["LeetCode"]},{"title":"leetcode刷题记录-20250808，day17，二叉树","path":"/2025/08/21/leetcode刷题记录-20250808，day17，二叉树/","content":"LeetCode 刷题记录 - 2025-08-08今天刷题总结如下： 题目 1: LeetCode 654.最大二叉树题目描述：给定一个不重复的整数数组 nums 。 最大二叉树 可以用下面的算法从 nums 递归地构建:创建一个根节点，其值为 nums 中的最大值。递归地在最大值 左边 的 子数组前缀上 构建左子树。递归地在最大值 右边 的 子数组后缀上 构建右子树。返回 nums 构建的 最大二叉树 。难度: Medium 思路二叉树题目一般考虑递归。对于递归，1、确定递归函数的参数和返回值；2、确定终止条件；3、确定单层递归的逻辑。这道题是要通过给定的数组创建最大二叉树，根节点是当前子数组的最大值，左子数组是构造左子树，右子数组构造右子树。单层递归就是要去找到最大值以及最大值的索引，将最大值赋值给当前根节点，通过索引划分左右子数组。当数组为空的时候递归结束。💡 OS： 要注意划分左右子数组时的切分点，因为切分点已经被赋给当前的根节点，所以左右子数组都不包括切分点。 代码如下class Solution public: TreeNode* constructMaximumBinaryTree(vectorint nums) if (nums.empty()) return nullptr; int index = 0; int max = nums[0]; for (int i = 0; i nums.size(); i++) if (max nums[i]) max = nums[i]; index = i; TreeNode* root = new TreeNode(max); if (nums.size() == 1) return root; // 切分左右子树时注意切分点，不包括切分点！！ vectorint numsLeft(nums.begin(), nums.begin()+index); vectorint numsRight(nums.begin()+index+1, nums.end()); root-left = constructMaximumBinaryTree(numsLeft); root-right = constructMaximumBinaryTree(numsRight); return root; ; 题目 2: LeetCode 617.合并二叉树 题目描述：给你两棵二叉树： root1 和 root2 。 想象一下，当你将其中一棵覆盖到另一棵之上时，两棵树上的一些节点将会重叠（而另一些不会）。你需要将这两棵树合并成一棵新二叉树。合并的规则是：如果两个节点重叠，那么将这两个节点的值相加作为合并后节点的新值；否则，不为 null 的节点将直接作为新二叉树的节点。 返回合并后的二叉树。 注意: 合并过程必须从两个树的根节点开始。难度: Easy 思路二叉树题目一般考虑递归。对于递归，1、确定递归函数的参数和返回值；2、确定终止条件；3、确定单层递归的逻辑。这个题合并二叉树，不难，就把两个数都遍历一遍就行了，考虑全合并时的情况就好了。 代码如下class Solution public: TreeNode* mergeTrees(TreeNode* root1, TreeNode* root2) if (!root1 !root2) return nullptr; if (!root1 root2) TreeNode* root = new TreeNode(root2-val); root-left = mergeTrees(nullptr, root2-left); root-right = mergeTrees(nullptr, root2-right); return root; if (root1 !root2) TreeNode* root = new TreeNode(root1-val); root-left = mergeTrees(root1-left, nullptr); root-right = mergeTrees(root1-right, nullptr); return root; TreeNode* root = new TreeNode(root1-val + root2-val); root-left = mergeTrees(root1-left, root2-left); root-right = mergeTrees(root1-right, root2-right); return root; ; 题目 3: LeetCode 700.二叉搜索树中的搜索 题目描述：给定二叉搜索树（BST）的根节点 root 和一个整数值 val。 你需要在 BST 中找到节点值等于 val 的节点。 返回以该节点为根的子树。 如果节点不存在，则返回 null 。难度: Easy 思路二叉树题目一般考虑递归。对于递归，1、确定递归函数的参数和返回值；2、确定终止条件；3、确定单层递归的逻辑。给定二叉搜索树和整数值，二叉搜索树就是根节点的值大于左孩子，小于右孩子，就是判断一下当前节点的值和给定的整数的大小关系。💡 OS： 代码如下class Solution public: TreeNode* searchBST(TreeNode* root, int val) TreeNode* next; if (!root) return nullptr; else if (root-val != val) next = (root-val val) ? root-left : root-right; else if (root-val == val) return root; return searchBST(next, val); ; 题目 4: LeetCode 98.验证二叉搜索树 题目描述：给你一个二叉树的根节点 root ，判断其是否是一个有效的二叉搜索树。 有效 二叉搜索树定义如下：节点的左子树只包含 严格小于 当前节点的数。节点的右子树只包含 严格大于 当前节点的数。所有左子树和右子树自身必须也是二叉搜索树。难度: Medium 思路二叉树题目一般考虑递归。对于递归，1、确定递归函数的参数和返回值；2、确定终止条件；3、确定单层递归的逻辑。判断给定的二叉树是否是有效的二叉搜索树，题目也给出了有效二叉搜索树定义。一开始没有考虑到OS中的情况，只判断了当前节点与当前节点的左右孩子。所以正确解法是：维护一个最大值最小值，遍历左子树时更新最大值，遍历右子树时更新最小值。节点值不满足 [min_val, max_val] 这个区间时返回false。💡 OS： 要注意左子树的所有节点的值都要严格小于根节点的值，右子树所有节点的值都要严格大于根节点的值！ 代码如下class Solution public: // 让左右子树各个结点的值都在一个范围内！！！ bool travelTree(TreeNode* root, long min_val, long max_val) if (!root) return true; if (root-val = min_val || root-val = max_val) return false; return travelTree(root-left, min_val, root-val) travelTree(root-right, root-val, max_val); bool isValidBST(TreeNode* root) return travelTree(root, LONG_MIN, LONG_MAX); ;","tags":["刷题","数据结构","算法","二叉树"],"categories":["LeetCode"]},{"title":"RK3588 平台部署 YOLO11-Seg","path":"/2025/08/21/RK3588-平台-YOLO11-Seg-部署与优化/","content":"本文记录了在 RK3588 开发板上部署 YOLO11-Seg 的流程，涵盖环境准备、模型转换与推理测试。 1. 环境准备此处列出的是我使用的相关环境，由于使用pytorch较多，所以训练框架默认pytorch。PC端需安装pytorch相关环境，环境安装及模型训练不过多说明（因为不是训练调参分享，哈哈）。官方文档环境要求如下，但是经过我实际使用发现，Ubuntu 16.04.7 LT + Python 3.10也可用。 PC端Ubuntu 16.04.7 LTPython 3.10.18Pytorch 2.2.0CUDA 12.1cmake 3.18.4aarch64-linux-gnu-gcc 6.2.1aarch64-linux-gnu-g++ 6.2.1RKNN-Toolkit2 v2.3.2 # 安装 RKNN-Toolkit2# 请根据不同的 python 版本及处理器架构，选择不同的 wheel 安装包文件：# 其中 x.x.x 是 RKNN-Toolkit2 版本号，cpxx 是 python 版本号pip install packages/x86_64/rknn_toolkit2-x.x.x-cpxx-cpxx-manylinux_2_17_x86_64.manylinux2014_x86_64.whl 板端# 查询 NPU 驱动版本dmesg | grep -i rknpu 或cat /sys/kernel/debug/rknpu/version 或cat /sys/kernel/debug/rknpu/driver_version 或cat /proc/debug/rknpu/driver_version# 显示 RKNPU driver: vX.X.X# 查询rknn_server版本strings /usr/bin/rknn_server | grep -i rknn_server version# 显示 rknn_server version: X.X.X# 查询librknnrt.so库版本strings /usr/lib/librknnrt.so | grep -i librknnrt version# 显示 librknnrt version: X.X.X 2. 模型转换模型转换分为两步，第一步由训练好的.pt转为.onnx，第二步从.onnx转为RKNPU可用的.rknn格式模型。模型训练框架使用的是YOLO11-Seg官方的版本(Ultralytics)。转换onnx同样有两种方式，第一种是利用Ultralytics提供直接导出.rknn格式的接口，我暂时未使用，大家可以试一下。第二种方式用rockchip提供的YOLO11代码导出(https://github.com/airockchip/ultralytics_yolo11). # 调整 ./ultralytics/cfg/default.yaml 中 model 文件路径，默认为 yolo11n.pt，若自己训练模型，请调接至对应的路径。python ./ultralytics/engine/exporter.py# 执行完毕后，会生成 ONNX 模型. 假如原始模型为 yolo11n.pt，则生成 yolo11n.onnx 模型。 rockchip修改了输出信息。官方版本导出onnx后只有一个输出[1, 84, 8400], 下图是通过rockchip提供代码导出。 转换onnx完成后，可以测试推理效果，一般情况下与pt结果一致。测试无误后利用rknn的api进行rknn模型转换，这里仅贴部分代码，完整代码会在仿真测试后给出。 # 创建 RKNN 对象rknn = RKNN(verbose=False)# 参数设置，这里做了归一化，在后续C代码中就不需要了，目标平台根据自己需求选择。rknn.config(mean_values=[[0, 0, 0]], std_values=[[255, 255, 255]], target_platform=platform)# 加载 onnx 模型ret = rknn.load_onnx(model=model_path)# 构建，这里可选是否对模型做量化，RK3588上只支持i8以及fp16格式。ret = rknn.build(do_quantization=do_quant, dataset=DATASET_PATH)# 开导ret = rknn.export_rknn(output_path) 3. 仿真测试仿真正确是上版正确的前提！！话不多说，直接上代码。 # 在仿真时这个的target要设置为None。ret = rknn.init_runtime(target=None, device_id=None, perf_debug=False)# 这里的 img 要经过前处理，比如补尺寸（letterbox），注意通过cv2.imread读进来的图片默认是BGR格式，如果你的模型接收的格式是RGB需要做转换outputs = rknn.inference(inputs=[img]) 完整代码！！ 点击展开代码 import osimport sysimport numpy as npfrom rknn.api import RKNNrealpath = os.path.abspath(__file__)_sep = os.path.seprealpath = realpath.split(_sep)sys.path.append(os.path.join(realpath[0]+_sep, *realpath[1:realpath.index(rknn_model_zoo-main)+1]))from py_utils.coco_utils import COCO_test_helperimport cv2from yolo11_seg import merge_seg, post_processDATASET_PATH = xxxDEFAULT_RKNN_PATH = xxxDEFAULT_QUANT = Trueimg_path = xxxdef parse_arg(): if len(sys.argv) 3: print(Usage: python3 onnx_model_path [platform] [dtype(optional)] [output_rknn_path(optional)].format(sys.argv[0])); print( platform choose from [rk3562, rk3566, rk3568, rk3576, rk3588, rv1126b, rv1109, rv1126, rk1808]) print( dtype choose from [i8, fp] for [rk3562, rk3566, rk3568, rk3576, rk3588, rv1126b]) print( dtype choose from [u8, fp] for [rv1109, rv1126, rk1808]) exit(1) model_path = sys.argv[1] platform = sys.argv[2] do_quant = DEFAULT_QUANT if len(sys.argv) 3: model_type = sys.argv[3] if model_type not in [i8, u8, fp]: print(ERROR: Invalid model type: .format(model_type)) exit(1) elif model_type in [i8, u8]: do_quant = True else: do_quant = False if len(sys.argv) 4: output_path = sys.argv[4] else: output_path = DEFAULT_RKNN_PATH return model_path, platform, do_quant, output_pathif __name__ == __main__: model_path, platform, do_quant, output_path = parse_arg() # 创建 RKNN 对象 rknn = RKNN(verbose=False) # 参数设置，这里做了归一化，在后续C代码中就不需要了，目标平台根据自己需求选择。 print(-- Config model) rknn.config(mean_values=[[0, 0, 0]], std_values=[[255, 255, 255]], target_platform=platform) print(done) # 加载 onnx 模型 print(-- Loading model) ret = rknn.load_onnx(model=model_path) if ret != 0: print(Load model failed!) exit(ret) print(done) # 构建，这里可选是否对模型做量化，RK3588上只支持i8以及fp16格式。 print(-- Building model) ret = rknn.build(do_quantization=do_quant, dataset=DATASET_PATH) if ret != 0: print(Build model failed!) exit(ret) print(done) # 在仿真时这个的target要设置为None。 print(-- Init runtime environment) ret = rknn.init_runtime(target=None, device_id=None, perf_debug=False) if ret != 0: print(Init runtime environment failed!) exit(ret) print(done) # 这里可选仿真精度分析，可以查看每一层结果差异 # # print(-- Accuracy analysis) # Ret = rknn.accuracy_analysis(inputs=[img_path], # target=None) # if ret != 0: # print(Accuracy analysis failed!) # exit(ret) # print(done) # 开导 print(-- Export rknn model) ret = rknn.export_rknn(output_path) if ret != 0: print(Export rknn model failed!) exit(ret) print(done) # 官方提供了接口直接生成C代码，简化了开发流程，可以在此基础上进行二次开发，目前我用这部分代码只测试精度。 # print(-- codegen) # ret = rknn.codegen(output_path=./rknn_app_demo, # inputs=[img_path], overwrite=True) # if ret != 0: # print(Export rknn model failed!) # exit(ret) # print(done) IMG_SIZE = (640, 640) # (width, height) co_helper = COCO_test_helper(enable_letter_box=True) img_src = cv2.imread(img_path) img = co_helper.letter_box(im= img_src.copy(), new_shape=(IMG_SIZE[1], IMG_SIZE[0]), pad_color=(114, 114, 114)) img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) img = np.expand_dims(img, 0); # 这里的 img 要经过前处理，比如补尺寸（letterbox），注意通过cv2.imread读进来的图片默认是BGR格式，如果你的模型接收的格式是RGB需要做转换 print(-- Running model) outputs = rknn.inference(inputs=[img]) print(done) boxes, classes, scores, seg_img = post_process(outputs) if boxes is not None: real_boxs = co_helper.get_real_box(boxes) real_segs = co_helper.get_real_seg(seg_img) img_p = merge_seg(img_src, real_segs, classes) img_output_path = ./xxx if not os.path.exists(img_output_path): os.mkdir(img_output_path) result_path = os.path.join(img_output_path, xxx.png) cv2.imwrite(result_path, img_p) print(saved to .format(result_path)) # Release rknn.release() 4. 编译及上版测试编写C代码，只是想测试的话可以直接使用官方提供的yolov8-seg的demo，因为前后处理是一样的操作，后续我会把我的代码链接放进来（其实是还没整理好，太乱了）。PC端测试、仿真测试以及上版测试结果如下。后续会更新优化部分。","tags":["RK3588","YOLO11","目标检测","实例分割"],"categories":["AI部署"]},{"title":"友链","path":"/friends/index.html","content":"友链关于小伙伴们如果宇宙中真有什么终极的逻辑，那就是我们终有一天会在舰桥上重逢，直到生命终结。 [2023-12] 友链失联了怎么办? 添加友链后如果网站长期无法访问，可能会被取消友链！如果您的网站恢复了，可以在申请友链时创建的那条 issue 中评论告知。 朋友们近期的文章 如何交换友链？ 您的网站应满足以下全部条件： 安全合规：合法的、非营利性、无木马植入的 HTTPS 站点。 非空壳网站：网站内发布至少 五篇 原创文章，内容题材不限。 我们需要有一定的有效互动： 先友后链：与博主有至少 半年 的有效互动，例如 issue 或者评论留言。 [2023-12] 友链申请条件变更说明 降低了对商业广告的要求，可以有但是不能太多。提高了「有效互动」的定义：5次更改为半年。 我已满足全部条件，快告诉我如何交换友链！ 如果您没有满足上述条件，即时提交了申请也不会通过哦～ 第一步：新建 Issue新建 GitHub Issue 按照模板格式填写并提交。为了提高图片加载速度，建议优化头像：打开 压缩图 上传自己的头像，将图片尺寸调整到 144px 后下载。将压缩后的图片上传到 去不图床 或者其它稳定的图床并使用此图片链接作为头像。第二步：添加友链并等待管理员审核请添加本站到您的友链中：title: xxxurl: https://xxx.comavatar: screenshot: 待管理员审核通过，添加了 active 标签后，回来刷新即可生效。如果您需要更新自己的友链，请直接修改 issue 内容，大约 3 分钟内生效，无需等待博客重新部署。"},{"title":"探索","path":"/explore/index.html","content":"…"},{"title":"关于","path":"/about/index.html","content":"友链关于昵称简介 关于本站 本站没有任何推广和打赏链接，如果您觉得哪个作品不错，欢迎去对应的仓库点个赞，或者在对应的文章下面留言互动一下。 开源项目无任何盈利目的，只在工作闲暇时间进行维护，有相关需求请前往对应项目提 Issue 进行反馈，通过私人邮件询问开源项目问题可能得不到答复。"},{"title":"朋友文章","path":"/friends/rss/index.html","content":""},{"title":"收藏","path":"/bookmark/index.html","content":"…"},{"title":"Page","path":"/page/index.html","content":"This is a page test."}]